{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "94be9ff1-ca88-4be0-a32e-cb616d923484"
   },
   "source": [
    "## #1 Wikipedia Page View API<span class=\"mw-editsection\" style=\"box-sizing: border-box; user-select: none; font-size: 0.875rem; margin-left: 1rem; vertical-align: baseline; line-height: 1em; margin-right: 0px;\"><span class=\"mw-editsection-bracket\" style=\"box-sizing: border-box;\">[</span><a href=\"https://wiki.communitydata.science/index.php?title=Community_Data_Science_Course_(Spring_2023)/Week_5_coding_challenges&amp;action=edit&amp;section=1\" title=\"Edit section: #1 Wikipedia Page View API\" style=\"box-sizing: border-box; color: rgb(27, 89, 155); background: none transparent;\">edit</a><span class=\"mw-editsection-bracket\" style=\"box-sizing: border-box;\">]</span></span>\n",
    "\n",
    "1. Identify a famous person who has been famous for at least a few years and that you have some personal interest in. Use the Wikimedia API to collect page view data from the English Wikipedia article on that person. Now use that data to generate a time-series visualization and include a link to it in your notebook.\n",
    "2. Identify 2 other languages editions of Wikipedia that have articles on that person. Collect page view data on the article in other languages and create a single visualization that shows how the dynamics and similar and/or different. (Note: My approach involved creating a TSV file with multiple columns.)\n",
    "3. Collect page view data on the articles about [Marvel Comics](https:\\en.wikipedia.org\\wiki\\Marvel_Comics) and [DC Comics](https:\\en.wikipedia.org\\wiki\\DC_Comics) in English Wikipedia. (If you'd rather replace these examples with some other comparison of popular rivals, that's just as good!)\n",
    "    1. Which has more total page views in 2022?\n",
    "    2. Can you draw a visualization in a spreadsheet that shows this? (Again, provide a link.)\n",
    "    3. Where there years since 2015 when the less viewed page was viewed more? How many and which ones?\n",
    "    4. Where their any months was this true? How many and which ones?\n",
    "    5. How about any days? How many?\n",
    "4. I've made [this file available](https:\\github.com\\kayleachampion\\spr23_CDSW\\blob\\main\\curriculum\\week5\\list_of_washington_alternative_rocks_bands_wikipedia-2023-04-25.jsonl) which includes list of more than 100 Wikipedia articles about alternative rock bands from Washington state that I built from [this category in Wikipedia](https:\\en.wikipedia.org\\wiki\\Category:Alternative_rock_groups_from_Washington_(state)).\\[\\*\\] It's a `.jsonl` file. Download the file (click \"raw\" and then save the file onto your drive). Now read it in, and request monthly page view data from all of them. If you need some help with loading it in, I've included some sample code at the bottom of this page.\n",
    "    1. Once you've done this, sum up all of the page views from all of the pages and print out a TSV file with these total numbers.\n",
    "    2. You know the routine by now! Now, make a time series graph of these numbers and include a link in your notebook.\n",
    "\n",
    "| <br> |  |\n",
    "| :-: | --- |\n",
    "|  | <br> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "azdata_cell_guid": "7b35130c-b939-4e61-b3c6-3f2bff5f6300",
    "is_executing": true,
    "language": "python",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "#From https://github.com/kayleachampion/spr23_CDSW/blob/main/curriculum/week5/week_5_lecture_part_1-data_collection.ipynb\n",
    "def get_wikipedia_pageviews(page_title, region_code):\n",
    "    # /metrics/pageviews/per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}\n",
    "    url = (\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\" + \n",
    "           f\"{region_code}.wikipedia.org/all-access/user/{page_title}/daily/20010101/20230424\")\n",
    "\n",
    "#Wikipedia API documentation mentions that User-Agent header needs to be unique.\n",
    "    headers = {\n",
    "        'User-Agent': 'python data collection bot by makohill@uw.edu'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if not response.status_code == 200:\n",
    "        print(\"ERROR, request not OK\")\n",
    "        \n",
    "    data = response.json()\n",
    "\n",
    "    return data\n",
    "\n",
    "#Modifying previous eg.\n",
    "\n",
    "def get_wikipedia_item(page_title):\n",
    "    url = f\"https://en.wikipedia.org/wiki/{page_title}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if not response.status_code == 200:\n",
    "        print(\"ERROR, request not OK\")\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_json_file(file_name_json, jsondata):\n",
    "    with open(file_name_json, \"w\") as my_file:\n",
    "        json_string = json.dumps(jsondata) #converts into json string        \n",
    "        print(json_string, file=my_file) #This was saved as a long string, not as spreadsheet as we had on the past.\n",
    "    \n",
    "    print(f\"{file_name_json} created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "azdata_cell_guid": "2e12c7e5-60a1-4fb3-9bee-df4187008ae0",
    "language": "python",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsondata:<class 'dict'>\n",
      "Taylor_Swift_wikipedia_pageviews.json created\n"
     ]
    }
   ],
   "source": [
    "#call to wikipedia\n",
    "#get page view data\n",
    "#get time-series visualization and include a multiple columns?\n",
    "\n",
    "#1.Identify a famous person who has been famous for at least a few years and that you have some personal interest in. Use the Wikimedia API \n",
    "#to collect page view data from the English Wikipedia article on that person. Now use that data to generate a time-series visualization \n",
    "#and include a link to it in your notebook.\n",
    "\n",
    "famous_person = \"Taylor_Swift\"\n",
    "jsondata = get_wikipedia_pageviews(famous_person,\"en\")\n",
    "file_name = f\"{famous_person}_wikipedia_pageviews\"\n",
    "file_name_json = file_name+\".json\"\n",
    "#get_wikipedia_item(famous_person)\n",
    "print (\"jsondata:\"+str(type(jsondata)))\n",
    "\n",
    "create_json_file(file_name_json,jsondata)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "ce4259f4-fdca-415d-bb7c-0fb1c379edbe",
    "language": "python",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'project': 'en.wikipedia', 'article': 'Taylor_Swift', 'granularity': 'daily', 'timestamp': '2015070100', 'access': 'all-access', 'agent': 'user', 'views': 35553}\n",
      "2855\n"
     ]
    }
   ],
   "source": [
    "#Way 1 to read file. \n",
    "#Reads whole file and saves it on \n",
    "\n",
    "def open_json_to_dict(file_name):\n",
    "    with open(file_name, 'r') as input_file:\n",
    "        input_data = input_file.read()\n",
    "        new_data = json.loads(input_data) #load into new_data so we won't have to work as big string.\n",
    "        return new_data\n",
    "\n",
    "\n",
    "new_data= open_json_to_dict(file_name_json)\n",
    "\n",
    "print(new_data['items'][0]) #check dictionary item \n",
    "\n",
    "print(len(new_data['items']))\n",
    "\n",
    "#Way 2 to read file\n",
    "#Reads each line and saves it on dictionary\n",
    "# day_dicts = []\n",
    "# with open(file_name_json, 'r') as f:\n",
    "#     for line in f.readlines():\n",
    "#         json_data2 = json.loads(line)\n",
    "#         day_dicts.extend(json_data2[\"items\"])\n",
    "\n",
    "# print(type(day_dict))\n",
    "# len(day_dicts)\n",
    "# for key in day_dict:\n",
    "#     print(key)\n",
    "#     #break;\n",
    "\n",
    "# print(day_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "azdata_cell_guid": "944a6228-4c1d-4a22-a5a0-29efc6759695",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def clean_up_timestamp(day):\n",
    "    new_time_stamp = day[0:4] + \"-\" + day[4:6] + \"-\" + day[6:8]\n",
    "    return new_time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "azdata_cell_guid": "d53f6dbb-f63a-4f58-867e-d64e5288bb2b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "#Clean up day format of each of the previous json items and save it on views_by_day\n",
    "\n",
    "def get_views_by_day(json_data):\n",
    "    #modify json data into a dictionary {timestamp:views}\n",
    "    views_by_day = {}\n",
    "    for day_dict in new_data['items']:\n",
    "        day = clean_up_timestamp(day_dict['timestamp'])\n",
    "        views_by_day[day] = day_dict['views']\n",
    "    return views_by_day\n",
    "\n",
    "views_by_day = get_views_by_day(new_data)\n",
    "\n",
    "# total_views_by_day = {} \n",
    "# for day_dict in day_dicts:\n",
    "#     day = clean_up_timestamp(day_dict[\"timestamp\"])\n",
    "#     if day in total_views_by_day:\n",
    "#         total_views_by_day[day] = total_views_by_day[day] + day_dict['views']\n",
    "#     else:\n",
    "#         total_views_by_day[day] = day_dict[\"views\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "azdata_cell_guid": "a87d9a26-a544-4a93-8b49-43326516bff7",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taylor_Swift_views_by_day_combo.tsv created\n"
     ]
    }
   ],
   "source": [
    "#turn into a tsv file\n",
    "def create_tsv_file(file_name, dictionary_data):\n",
    "    with open(file_name, \"w\") as f:\n",
    "        print(\"day\\ttotal_views\", file=f)\n",
    "        \n",
    "        for day in dictionary_data.keys():\n",
    "            views = dictionary_data[day]\n",
    "            print(day, \"\\t\", views, file=f)\n",
    "\n",
    "    print(f\"{file_name} created\")\n",
    "\n",
    "file_name = f\"{famous_person}_views_by_day_combo.tsv\"\n",
    "create_tsv_file(file_name, views_by_day)\n",
    "\n",
    "# with open(f\"{famous_person}_views_by_day_combo2.tsv\", \"w\") as f:\n",
    "#     print(\"day\\ttotal_views\", file=f)\n",
    "    \n",
    "#     for day in total_views_by_day.keys():\n",
    "#         views = total_views_by_day[day]\n",
    "#         print(day, \"\\t\", views, file=f)\n",
    "\n",
    "# print(f\"{famous_person}_views_by_day_combo2.tsv created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "azdata_cell_guid": "b238c96d-f531-47a3-b7e6-39a0aecc6d88",
    "language": "python",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple_Taylor_Swift_wikipedia_pageviews.json created\n",
      "multiple_Taylor_Swift_wikipedia_pageviews_views_by_day_combo.tsv created\n"
     ]
    }
   ],
   "source": [
    "#Identify 2 other languages editions of Wikipedia that have articles on that person. \n",
    "#Collect page view data on the article in other languages and create a single visualization that shows how the dynamics and similar and/or different. \n",
    "#(Note: My approach involved creating a TSV file with multiple columns.)\n",
    "\n",
    "region_codes = [\"en\",\"es\", \"ja\"]\n",
    "multiple_code_file =f\"multiple_{famous_person}_wikipedia_pageviews\"\n",
    "\n",
    "view_data = []\n",
    "for code in region_codes:\n",
    "    code_jsondata = get_wikipedia_pageviews(famous_person,code)\n",
    "    code_json_data = code_jsondata[\"items\"]\n",
    "    #print(code_json_data)\n",
    "    view_data = view_data + code_json_data\n",
    "\n",
    "#print(view_data)\n",
    "\n",
    "create_json_file(multiple_code_file+\".json\", view_data)\n",
    "\n",
    "new_data = open_json_to_dict(multiple_code_file+\".json\")\n",
    "\n",
    "views_by_day = {}\n",
    "for day_dict in new_data:\n",
    "    day = clean_up_timestamp(day_dict['timestamp'])\n",
    "    views_by_day[day] = day_dict['views']\n",
    "\n",
    "#turn into a tsv file\n",
    "with open(f\"{multiple_code_file}.tsv\", \"w\") as f:\n",
    "    print(\"project\\tday\\ttotal_views\", file=f)\n",
    "\n",
    "    for day_dict in new_data:\n",
    "        day = clean_up_timestamp(day_dict['timestamp'])\n",
    "        views = day_dict['views']\n",
    "        project = day_dict['project']\n",
    "        print(day,\"\\t\",project,\"\\t\",views,file=f)\n",
    "\n",
    "\n",
    "print(f\"{multiple_code_file}_views_by_day_combo.tsv created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "azdata_cell_guid": "11048a2d-8cd1-45ab-a8c8-bcaed1cdc948",
    "language": "python",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: Marvel\n",
      "2015-1 DC\n",
      "2015-2 DC\n",
      "2015-3 DC\n",
      "2015-4 DC\n",
      "2015-5 DC\n",
      "2015-6 DC\n",
      "2015-7 DC\n",
      "2015-8 DC\n",
      "2015-9 DC\n",
      "2015-10 DC\n",
      "2015-11: Marvel\n",
      "2016: DC\n",
      "2016-1 DC\n",
      "2016-2 DC\n",
      "2016-3 DC\n",
      "2016-4 DC\n",
      "2016-5 DC\n",
      "2016-6 DC\n",
      "2016-7 DC\n",
      "2016-8 DC\n",
      "2016-9 DC\n",
      "2016-10: Marvel\n",
      "2016-11: Marvel\n",
      "2017: Marvel\n",
      "2017-1 DC\n",
      "2017-2 DC\n",
      "2017-3 DC\n",
      "2017-4 DC\n",
      "2017-5 DC\n",
      "2017-6 DC\n",
      "2017-7 DC\n",
      "2017-8 DC\n",
      "2017-9 DC\n",
      "2017-10 DC\n",
      "2017-11 DC\n",
      "2018: Marvel\n",
      "2018-1 DC\n",
      "2018-2 DC\n",
      "2018-3 DC\n",
      "2018-4 DC\n",
      "2018-5 DC\n",
      "2018-6 DC\n",
      "2018-7 DC\n",
      "2018-8 DC\n",
      "2018-9 DC\n",
      "2018-10: Marvel\n",
      "2018-11: Marvel\n",
      "2019: Marvel\n",
      "2019-1 DC\n",
      "2019-2 DC\n",
      "2019-3 DC\n",
      "2019-4 DC\n",
      "2019-5 DC\n",
      "2019-6 DC\n",
      "2019-7 DC\n",
      "2019-8 DC\n",
      "2019-9 DC\n",
      "2019-10 DC\n",
      "2019-11 DC\n",
      "2020: DC\n",
      "2020-1 DC\n",
      "2020-2 DC\n",
      "2020-3 DC\n",
      "2020-4 DC\n",
      "2020-5 DC\n",
      "2020-6 DC\n",
      "2020-7 DC\n",
      "2020-8 DC\n",
      "2020-9 DC\n",
      "2020-10: Marvel\n",
      "2020-11 DC\n",
      "2021: Marvel\n",
      "2021-1 DC\n",
      "2021-2 DC\n",
      "2021-3 DC\n",
      "2021-4 DC\n",
      "2021-5 DC\n",
      "2021-6 DC\n",
      "2021-7 DC\n",
      "2021-8 DC\n",
      "2021-9 DC\n",
      "2021-10: Marvel\n",
      "2021-11: Marvel\n",
      "2022: Marvel\n",
      "2022-1 DC\n",
      "2022-2 DC\n",
      "2022-3 DC\n",
      "2022-4 DC\n",
      "2022-5 DC\n",
      "2022-6 DC\n",
      "2022-7 DC\n",
      "2022-8 DC\n",
      "2022-9 DC\n",
      "2022-10 DC\n",
      "2022-11 DC\n"
     ]
    }
   ],
   "source": [
    "#3. Collect page view data on the articles about Marvel Comics and DC Comics in English Wikipedia. \n",
    "#(If you'd rather replace these examples with some other comparison of popular rivals, that's just as good!)\n",
    "\n",
    "marvel_jsondata = get_wikipedia_pageviews(\"Marvel_Comics\",\"en\")\n",
    "dc_jsondata= get_wikipedia_pageviews(\"DC_Comics\",\"en\")\n",
    "\n",
    "marvel_by_day = {}\n",
    "dc_by_day = {}\n",
    "marvel_by_year = {}\n",
    "dc_by_year={}\n",
    "\n",
    "for _day in marvel_jsondata['items']:\n",
    "    cleaned_day = clean_up_timestamp(_day[\"timestamp\"])    \n",
    "    current_year = _day[\"timestamp\"][0:4]\n",
    "    current_month = current_year+\"-\"+_day[\"timestamp\"][4:6]\n",
    "    current_day = _day[\"timestamp\"][6:8]\n",
    "\n",
    "    marvel_by_year[cleaned_day] = _day[\"views\"]\n",
    "    if (current_month in marvel_by_year):\n",
    "        marvel_by_year[current_month] = marvel_by_year[current_month]+_day[\"views\"]\n",
    "    else:\n",
    "        marvel_by_year[current_month] = _day[\"views\"]\n",
    "        \n",
    "    if (current_year in marvel_by_year):        \n",
    "        marvel_by_year[current_year]  = marvel_by_year[current_year]+_day[\"views\"]\n",
    "    else:\n",
    "        marvel_by_year[current_year]  = _day[\"views\"]\n",
    "\n",
    "\n",
    "for _day in dc_jsondata['items']:\n",
    "    cleaned_day = clean_up_timestamp(_day[\"timestamp\"])    \n",
    "    current_year = _day[\"timestamp\"][0:4]\n",
    "    current_month = current_year+\"-\"+_day[\"timestamp\"][4:6]\n",
    "    current_day = _day[\"timestamp\"][6:8]\n",
    "\n",
    "    dc_by_year[cleaned_day] = _day[\"views\"]\n",
    "    if (current_month in dc_by_year):\n",
    "        dc_by_year[current_month] = dc_by_year[current_month]+_day[\"views\"]\n",
    "    else:\n",
    "        dc_by_year[current_month] = _day[\"views\"]\n",
    "\n",
    "    if (current_year in dc_by_year):        \n",
    "        dc_by_year[current_year]  = dc_by_year[current_year]+_day[\"views\"]\n",
    "    else:\n",
    "        dc_by_year[current_year]  = _day[\"views\"]\n",
    "\n",
    "    \n",
    "\n",
    "#print(\"Marvel Comics:\")\n",
    "#print(marvel_by_year)\n",
    "#print(\"DC Comics:\")\n",
    "#print(dc_by_year)\n",
    "\n",
    "with open(f\"views_by_day_comics.tsv\", \"w\") as f:\n",
    "    print(\"project\\tdate\\ttotal_views\", file=f)\n",
    "\n",
    "    for day_dict in new_data:\n",
    "        day = clean_up_timestamp(day_dict['timestamp'])\n",
    "        views = day_dict['views']\n",
    "        project = day_dict['project']\n",
    "        print(day,\"\\t\",project,\"\\t\",views,file=f)\n",
    "\n",
    "years = [\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\"]\n",
    "for year in years:\n",
    "    if (marvel_by_year[year] > dc_by_year[year]):\n",
    "        print(f\"{year}: Marvel\")\n",
    "    else:\n",
    "        print(f\"{year}: DC\")\n",
    "\n",
    "    for month in range(11):\n",
    "        value_marvel =0\n",
    "        value_dc= 0\n",
    "        if (f\"{year}-{month+1}\" in marvel_by_year):\n",
    "            value_marvel = marvel_by_year[f\"{year}-{month+1}\"]\n",
    "\n",
    "        if (f\"{year}-{month+1}\" in dc_by_year):\n",
    "            value_dc = dc_by_year[f\"{year}-{month+1}\"]\n",
    "\n",
    "        if (value_marvel>value_dc):\n",
    "            print(f\"{year}-{month+1}: Marvel\")\n",
    "        else:\n",
    "            print(f\"{year}-{month+1} DC\")\n",
    "\n",
    "#Can you draw a visualization in a spreadsheet that shows this? (Again, provide a link.)\n",
    "#Where there years since 2015 when the less viewed page was viewed more? How many and which ones?\n",
    "    # 2015: Marvel\n",
    "    # 2016: DC\n",
    "    # 2017: Marvel\n",
    "    # 2018: Marvel\n",
    "    # 2019: Marvel\n",
    "    # 2020: DC\n",
    "    # 2021: Marvel\n",
    "    # 2022: Marvel\n",
    "#Where their any months was this true? How many and which ones?\n",
    "#How about any days? How many?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "azdata_cell_guid": "8bda68ea-ced1-414f-8aec-fae6ecf2c785",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#4.I've made this file available which includes list of more than 100 Wikipedia articles about alternative rock bands from Washington state that \n",
    "#I built from this category in Wikipedia.[*] It's a .jsonl file. \n",
    "#Download the file (click \"raw\" and then save the file onto your drive). \n",
    "#Now read it in, and request monthly page view data from all of them. If you need some help with loading it in, \n",
    "#I've included some sample code at the bottom of this page.\n",
    "#Once you've done this, sum up all of the page views from all of the pages and print out a TSV file with these total numbers.\n",
    "#You know the routine by now! Now, make a time series graph of these numbers and include a link in your notebook.\n",
    "\n",
    "#page_dicts = raw_data[\"*\"][0]['a']['*']\n",
    "\n",
    "#with open(\"list_of_washington_alternative_rocks_bands_wikipedia-2023-04-25.jsonl\", 'w') as band_list:\n",
    "\n",
    "band_list = []\n",
    "with open(\"list_of_washington_alternative_rocks_bands_wikipedia-2023-04-25.jsonl\", 'r') as input_file:\n",
    "    for line in input_file.readlines():\n",
    "        json_data2 = json.loads(line)\n",
    "        #print(json_data2)\n",
    "        band_list.append(json_data2[\"page_title\"])\n",
    "\n",
    "# #print(band_list)\n",
    "\n",
    "band_items = []\n",
    "for band in band_list:\n",
    "    print(\".\")\n",
    "    band_views = get_wikipedia_pageviews(band,\"en\")    \n",
    "    band_items = band_items + band_views[\"items\"]\n",
    "\n",
    "\n",
    "\n",
    "#create_json_file(\"alternative_bands.json\",band_items)\n",
    "\n",
    "\n",
    "    \n",
    "    # for page in page_dicts:\n",
    "    #     print(page['title']) # also print it out so we can see it\n",
    "    #     output_dict = {'website' : 'en.wikipedia.org',\n",
    "    #                    'page_title' : page['title'] }\n",
    "    #     output_string = json.dumps(output_dict)\n",
    "    #     print(output_string, file=band_list) # print to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "azdata_cell_guid": "40d87f7c-a4a9-458f-a74f-a7df112fcd16",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alternative_bands.json created\n",
      "alternative_bands.tsv created\n"
     ]
    }
   ],
   "source": [
    "alternative_bands_file_name = \"alternative_bands\"\n",
    "create_json_file(alternative_bands_file_name+\".json\",band_items)\n",
    "bands_dict = open_json_to_dict(alternative_bands_file_name+\".json\")\n",
    "#print(bands_dict)\n",
    "\n",
    "views_by_day = {}\n",
    "with open(f\"{alternative_bands_file_name}.tsv\", \"w\") as f:\n",
    "    print(\"band\\tday\\ttotal_views\", file=f)\n",
    "    for day_dict in bands_dict:\n",
    "        day = clean_up_timestamp(day_dict['timestamp'])\n",
    "        band_name = bands_dict = day_dict[\"article\"]\n",
    "        views_by_day[day] = day_dict['views']\n",
    "        print(band_name,\"\\t\",day,\"\\t\",day_dict['views'],file=f)\n",
    "\n",
    "print(f\"{alternative_bands_file_name}.tsv created\")\n",
    "\n",
    "#get_views_by_day(bands_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9d709bd4-d43c-4724-9c03-57ce63c990f3"
   },
   "source": [
    "## #2 Starting on your projects<span class=\"mw-editsection\" style=\"box-sizing: border-box; user-select: none; font-size: 0.875rem; margin-left: 1rem; vertical-align: baseline; line-height: 1em; margin-right: 0px;\"><span class=\"mw-editsection-bracket\" style=\"box-sizing: border-box;\">[</span><a href=\"https://wiki.communitydata.science/index.php?title=Community_Data_Science_Course_(Spring_2023)/Week_5_coding_challenges&amp;action=edit&amp;section=2\" title=\"Edit section: #2 Starting on your projects\" style=\"color: rgb(27, 89, 155); box-sizing: border-box; background: none transparent;\">edit</a><span class=\"mw-editsection-bracket\" style=\"box-sizing: border-box;\">]</span></span>\n",
    "\n",
    "|  |  |\n",
    "| :-: | --- |\n",
    "| [![Cmbox notice.png](https://upload.wikimedia.org/wikipedia/commons/7/76/Cmbox_notice.png)](https:\\wiki.communitydata.science\\File:Cmbox_notice.png) | If you are planning on collecting data from Reddit, please look into using the [Pushshift API](https:\\pushshift.io\\) instead of the default Reddit API. The Pushshift API is not as up-to-date but it is targeted toward data scientists, not app-makers, and is likely much better suited to our needs in the class. That said, take a look at both! |\n",
    "\n",
    "In this section, you will take your first steps towards working with your project API. Many of these questions will not involve code, so just mark down your answers in cells in your notebook.\n",
    "\n",
    "One very useful trick is to convert cells into \"markdown\" mode. You can do in the menu with _Cell→Cell Type→Markdown_ or you can just type `m` when the cell is selected but not being edited (just press `Esc` if you are editing to switch out of edit mode). Clicking `y` turns it back into code. Markdown is just normal text but if you want to do fancier stuff like links or formatting you can look at this [Markdown Cheat Sheet](https:\\www.markdownguide.org\\cheat-sheet\\).\n",
    "\n",
    "Feel free to document any findings you think might be useful as you continue to work on your project; you might thank yourself later!\n",
    "\n",
    "1. Identify an API you will (or might!) want to use for your project.\n",
    "2. Find documentation for that API and include links in your notebook.\n",
    "3. What are the API endpoints you plan to use? What are the parameters you will need to use at that endpoint?\n",
    "4. Is there a Python module that exists that helps make contact with the API? (See if you can you find example code on how to use it).\n",
    "    1. If so, download it, install it, and import it into your notebook.\n",
    "5. Does the API require authentication? Does it need to be approved?\n",
    "    1. If so, sign up for a developer account and get your keys. (Do this early because it often takes time for these accounts to be approved.)\n",
    "6. Does the API list rate limits? Does it make any requests about how you should use it?\n",
    "7. Make a single API call, either directly using requests or using the Python module you have used. It doesn't matter for what. The goal is that you can get _something'_.\n",
    "8. IMPORTANT: If you have included any API keys in your notebook, _make a copy of your notebook, delete the cell where you include the keys, before you upload the copy of the notebook._ We'll show you some tricks for hiding this information going forward.\n",
    "\n",
    "## Notes<span class=\"mw-editsection\" style=\"box-sizing: border-box; user-select: none; font-size: 0.875rem; margin-left: 1rem; vertical-align: baseline; line-height: 1em; margin-right: 0px;\"><span class=\"mw-editsection-bracket\" style=\"box-sizing: border-box;\">[</span><a href=\"https://wiki.communitydata.science/index.php?title=Community_Data_Science_Course_(Spring_2023)/Week_5_coding_challenges&amp;action=edit&amp;section=3\" title=\"Edit section: Notes\" style=\"color: rgb(27, 89, 155); box-sizing: border-box; background: none transparent;\">edit</a><span class=\"mw-editsection-bracket\" style=\"box-sizing: border-box;\">]</span></span>\n",
    "\n",
    "\\[\\*\\] You will probably not be shocked to hear that I collected this data from an API! I've included a Jupyter Notebook with the code to grab that data from [the PetScan API](https:\\petscan.wmflabs.org\\) [in the form of this Github notebook](https:\\github.com\\kayleachampion\\spr23_CDSW\\blob\\main\\curriculum\\week5\\get_washington_alternative_rock_bands_list-20230425.ipynb).\n",
    "\n",
    "If you just want to read it in the file, remember it's just a JSONL file so you can modify the code from the lecture and it should work (e.g., something with `open()` and the `.readlines()` function associated with file variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. I´m looking forward to use IMDB API and/or rotten tomatoes API.\n",
    "#2. https://developer.imdb.com/documentation/api-documentation, https://developer.fandango.com/rotten_tomatoes\n",
    "#3. Not sure as the documentation is not completely clear about this since I don´t have access to these yet.\n",
    "#4. requests\n",
    "#5. Yes, authentication seems to be based on an API key.\n",
    "#6. I don't know yet as I don't have access yet.\n",
    "#7. NA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
