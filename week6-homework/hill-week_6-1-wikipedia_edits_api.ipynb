{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify a movie, television, video game, or other media property that has both (a) 5 or more related articles on Wikipedia and (b) 5 or more other articles on the same topic on a Fandom.com website. Any large entertainment franchise will definitely work but feel free to get creative! For example, you might choose 5 Wikipedia articles about the anime Naruto and 5 articles (pages) from the naruto.fandom.com site. You may notice that fandom.com has a top layer with staff-produced video content, but once you dig down into a particular fandom's wiki, you'll start to see a more familiar wiki style page. For example, compare the fandom.com page about the SpongeBob pilot episode 'Help Wanted' and the Wikipedia page about the same pilot episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are each main characters in The Wire\n",
    "articles = [\"Jimmy McNulty\",\n",
    "            \"Rhonda Pearlman\",\n",
    "            \"Stringer Bell\",\n",
    "            \"Omar Little\",\n",
    "            \"Tommy Carcetti\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_url = \"http://en.wikipedia.org/w/api.php/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_revision_json(endpoint, title):\n",
    "    api_answers = []\n",
    "    \n",
    "    parameters = {'action' : 'query',\n",
    "                  'titles' : title,\n",
    "                  'prop' : 'revisions',\n",
    "                  'rvprop' : 'flags|timestamp|user|size|ids',\n",
    "                  'rvlimit' : 500,\n",
    "                  'format' : 'json',\n",
    "                   }\n",
    "\n",
    "    # we'll repeat this forever (i.e., we'll only stop when we find\n",
    "    # the \"break\" command)\n",
    "    while True:\n",
    "        # this will wait for one second\n",
    "        # time.sleep(1)\n",
    "        \n",
    "        # the first line open the urls but also handles unicode urls\n",
    "        call = requests.get(endpoint, params=parameters)\n",
    "        api_answer = call.json()\n",
    "        \n",
    "        # now we'll add this to whatever we are tracking\n",
    "        api_answers.append(api_answer)\n",
    "        \n",
    "        # 'continue' tells us there's more revisions to add\n",
    "        if 'continue' in api_answer.keys():\n",
    "            # replace the 'continue' parameter with the contents of the\n",
    "            # api_answer dictionary.\n",
    "            parameters.update(api_answer['continue'])\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return(api_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now working on: Jimmy McNulty\n",
      "now working on: Rhonda Pearlman\n",
      "now working on: Stringer Bell\n",
      "now working on: Omar Little\n",
      "now working on: Tommy Carcetti\n"
     ]
    }
   ],
   "source": [
    "wp_api_answers = []\n",
    "with open('thewire_characters_wikipedia-20230508.jsonl', 'w') as output_file:\n",
    "    for page_title in articles:\n",
    "        \n",
    "        print(f\"now working on: {page_title}\")\n",
    "        page_api_answers = get_article_revision_json(wikipedia_url, page_title)\n",
    "        for api_answer in page_api_answers:\n",
    "            print(json.dumps(api_answer), file=output_file)\n",
    "            wp_api_answers.append(api_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First modify the code from first sets of notebooks I used in the Community Data Science Course (Spring 2023)/Week 6 lecture to download data (and metadata) about revisions to the 5 articles you chose from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_data_into_revisions(api_answers):\n",
    "    revisions = []\n",
    "\n",
    "    for api_answer in api_answers:\n",
    "\n",
    "        # get the list of pages from the json object\n",
    "        pages = api_answer[\"query\"][\"pages\"]\n",
    "\n",
    "        # for every page, (there should always be only one) get its revisions:\n",
    "        for page_id in pages.keys():\n",
    "            query_revisions = pages[page_id][\"revisions\"]\n",
    "            title = pages[page_id]['title']\n",
    "\n",
    "            # for every revision, first we do some cleaning up\n",
    "            for rev in query_revisions:\n",
    "                #print(rev)\n",
    "                # let's continue/skip this revision if the user is hidden\n",
    "                if \"userhidden\" in rev.keys():\n",
    "                    continue\n",
    "\n",
    "                # 1: add a title field for the article because we're going to mix them together\n",
    "                rev[\"title\"] = title\n",
    "\n",
    "                # 2: let's \"recode\" anon so it's true or false instead of present/missing\n",
    "                if \"anon\" in rev.keys():\n",
    "                    rev[\"anon\"] = True\n",
    "                else:\n",
    "                    rev[\"anon\"] = False\n",
    "\n",
    "                # 3: let's recode \"minor\" in the same way\n",
    "                if \"minor\" in rev.keys():\n",
    "                    rev[\"minor\"] = True\n",
    "                else:\n",
    "                    rev[\"minor\"] = False\n",
    "\n",
    "                # we're going to change the timestamp to make it work a little better in excel/spreadsheets\n",
    "                rev[\"timestamp\"] = rev[\"timestamp\"].replace(\"T\", \" \")\n",
    "                rev[\"timestamp\"] = rev[\"timestamp\"].replace(\"Z\", \"\")\n",
    "\n",
    "                # finally, save the revisions we've seen to a varaible\n",
    "                revisions.append(rev)\n",
    "                \n",
    "    return revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_revisions = api_data_into_revisions(wp_api_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be ready to share:\n",
    "\n",
    "(i) what proportion of those edits were made by users without accounts (\"anon\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prop_flag(revisions, flag):\n",
    "\n",
    "    num_edits = len(revisions)\n",
    "\n",
    "    # count the number of anonymous edits \n",
    "    num_flagged = 0\n",
    "\n",
    "    for rev in revisions:\n",
    "        if rev[flag]:\n",
    "            num_flagged = num_flagged + 1\n",
    "\n",
    "    prop_flagged = num_flagged / num_edits\n",
    "\n",
    "    print(f\"total edits: {num_edits}\")\n",
    "    print(f\"flaged edits: {num_flagged}\")\n",
    "    print(f\"proportion flagged: {prop_flagged}\")\n",
    "    return prop_flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total edits: 2275\n",
      "flaged edits: 1091\n",
      "proportion flagged: 0.47956043956043953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47956043956043953"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_flag(wp_revisions, \"anon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) what proportion of those edits were marked as \"minor\", and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total edits: 2275\n",
      "flaged edits: 499\n",
      "proportion flagged: 0.21934065934065933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21934065934065933"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_flag(wp_revisions, \"minor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iii) make and share a visualization of the total number of edits across those 5 articles over time (I didn't do this in class but I made the TSV file would allow this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_edits_by_day (revisions, output_filename):\n",
    "    # lets count the number of edits by day\n",
    "    edits_by_day = {}\n",
    "    for rev in revisions:\n",
    "        day_string = rev['timestamp'][0:10]\n",
    "\n",
    "        if day_string in edits_by_day.keys():\n",
    "            edits_by_day[day_string] = edits_by_day[day_string] + 1\n",
    "        else:\n",
    "            edits_by_day[day_string] = 1\n",
    "    \n",
    "    # write out a TSV file we could analyze in google docs\n",
    "    with open(output_filename, \"w\", encoding='utf-8') as output_file:\n",
    "        # write a header\n",
    "        print(\"date\\tedits\", file=output_file)\n",
    "\n",
    "        # iterate through every day and print out data into the file\n",
    "        for day_string in edits_by_day.keys():\n",
    "            print(\"\\t\".join([day_string, str(edits_by_day[day_string])]), file=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_edits_by_day(wp_revisions, \"thewire_characters_edits_by_day-enwp.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.google.com/spreadsheets/d/17slFCkD8EqiP6VKvojkb-BwpoQQvN6sZ9xGKh3erWX4/edit#gid=85977895"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now grab data for the 5 articles you chose from the Fandom.com wiki you identified and grab revision/edit data from there. (Hint: Your wikipedia work will give you lots of clues here: for example, the fandom API endpoint for The Wire is https://thewire.fandom.com/api.php and the Fandom API, as I said in class, is the same as the Wikipedia API). Produce answers to the same three questions (i, ii, and iii) above but using this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fandom_url = \"http://thewire.fandom.com/api.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now working on: Jimmy McNulty\n",
      "now working on: Rhonda Pearlman\n",
      "now working on: Stringer Bell\n",
      "now working on: Omar Little\n",
      "now working on: Tommy Carcetti\n"
     ]
    }
   ],
   "source": [
    "fd_api_answers = []\n",
    "with open('thewire_characters_fandom-20230508.jsonl', 'w') as output_file:\n",
    "    for page_title in articles:\n",
    "        \n",
    "        print(f\"now working on: {page_title}\")\n",
    "        page_api_answers = get_article_revision_json(fandom_url, page_title)\n",
    "        for api_answer in page_api_answers:\n",
    "            print(json.dumps(api_answer), file=output_file)\n",
    "            fd_api_answers.append(api_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_revisions = api_data_into_revisions(fd_api_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total edits: 158\n",
      "flaged edits: 28\n",
      "proportion flagged: 0.17721518987341772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17721518987341772"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_flag(fd_revisions, \"anon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) what proportion of those edits were marked as \"minor\", and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total edits: 158\n",
      "flaged edits: 19\n",
      "proportion flagged: 0.12025316455696203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12025316455696203"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_flag(fd_revisions, \"minor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_edits_by_day(fd_revisions, \"thewire_characters_edits_by_day-fandom.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.google.com/spreadsheets/d/17slFCkD8EqiP6VKvojkb-BwpoQQvN6sZ9xGKh3erWX4/edit#gid=964347220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i will just print 5 files and put them together in google sheets\n",
    "for title in articles:\n",
    "    with open(f\"article_size_per_day-{title}.tsv\", 'w') as output_file:\n",
    "        print(\"timestamp\\tsize\", file=output_file)\n",
    "        for rev in wp_revisions:\n",
    "            # skip if it's not the right article\n",
    "            if rev['title'] != title:\n",
    "                continue\n",
    "            print(f'{rev[\"timestamp\"]}\\t{rev[\"size\"]}', file=output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jimmy McNulty: https://docs.google.com/spreadsheets/d/17slFCkD8EqiP6VKvojkb-BwpoQQvN6sZ9xGKh3erWX4/edit#gid=1589826795erWX4/edit#gid=85977895"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rhonda Pearlman: https://docs.google.com/spreadsheets/d/17slFCkD8EqiP6VKvojkb-BwpoQQvN6sZ9xGKh3erWX4/edit#gid=924927859"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rhonda Pearlman: https://docs.google.com/spreadsheets/d/17slFCkD8EqiP6VKvojkb-BwpoQQvN6sZ9xGKh3erWX4/edit#gid=924927859"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stringer Bell: https://docs.google.com/spreadsheets/d/17slFCkD8EqiP6VKvojkb-BwpoQQvN6sZ9xGKh3erWX4/edit#gid=972117478"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omar Little: https://docs.google.com/spreadsheets/d/17slFCkD8EqiP6VKvojkb-BwpoQQvN6sZ9xGKh3erWX4/edit#gid=28443467"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tommy Carcetti: https://docs.google.com/spreadsheets/d/17slFCkD8EqiP6VKvojkb-BwpoQQvN6sZ9xGKh3erWX4/edit#gid=2014380976"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
