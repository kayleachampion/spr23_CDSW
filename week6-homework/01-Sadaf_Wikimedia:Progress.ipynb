{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc7dce5",
   "metadata": {},
   "source": [
    "Wikimedia API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b360b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6870cb7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as alien_pages.jsonl successfully.\n"
     ]
    }
   ],
   "source": [
    "#getting data on Alien Film Franchise from Wiki\n",
    "\n",
    "def get_wikipedia_pages():\n",
    "    search_term = \"Alien film franchise\"\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?action=query&format=json&list=search&srsearch={search_term}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    pages = []\n",
    "    \n",
    "    if \"query\" in data and \"search\" in data[\"query\"]:\n",
    "        search_results = data[\"query\"][\"search\"]\n",
    "        \n",
    "        for result in search_results:\n",
    "            title = result[\"title\"]\n",
    "            snippet = result[\"snippet\"]\n",
    "            page_id = result[\"pageid\"]\n",
    "            \n",
    "            # Create a dictionary for each page and add it to the list\n",
    "            page_info = {\n",
    "                \"title\": title,\n",
    "                \"snippet\": snippet,\n",
    "                \"page_id\": page_id\n",
    "            }\n",
    "            pages.append(page_info)\n",
    "    \n",
    "    return pages\n",
    "\n",
    "# Call the function to get the list of Wikipedia pages\n",
    "alien_pages = get_wikipedia_pages()\n",
    "\n",
    "# Save the data to a JSONL file\n",
    "with open(\"alien_pages.jsonl\", \"w\") as jsonl_file:\n",
    "    for page in alien_pages:\n",
    "        jsonl_file.write(json.dumps(page) + \"\\n\")\n",
    "\n",
    "print(\"Data saved as alien_pages.jsonl successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9a7038e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now working on: Alien (franchise)\n",
      "now working on: Untitled Alien film\n",
      "now working on: List of Alien (franchise) novels\n",
      "now working on: Alien vs. Predator\n",
      "now working on: Predator (franchise)\n",
      "now working on: Prometheus (2012 film)\n",
      "now working on: Alien: Covenant\n",
      "now working on: Alien vs. Predator (film)\n",
      "now working on: Aliens (film)\n",
      "now working on: Alien (film)\n"
     ]
    }
   ],
   "source": [
    "# writing data to a file\n",
    "\n",
    "with open('alien_pages.jsonl', 'r') as input_file,\\\n",
    "    open(\"GetAlienPages.jsonl\", 'w') as output_file:\n",
    "    \n",
    "    for line in input_file.readlines():\n",
    "        line_dict = json.loads(line)\n",
    "        page_title = line_dict[\"title\"]\n",
    "        \n",
    "        print(f\"now working on: {page_title}\")\n",
    "        api_answers = get_article_revision_json(page_title)\n",
    "        for api_answer in api_answers:\n",
    "            print(json.dumps(api_answer), file=output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "135a194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-coding so we can read the data \n",
    "revisions = []\n",
    "\n",
    "with open(\"GetAlienPages.jsonl\", 'r') as input_file:\n",
    "    for line in input_file.readlines():\n",
    "        api_answer = json.loads(line)\n",
    "        \n",
    "        pages = api_answer[\"query\"][\"pages\"]\n",
    "        \n",
    "        for page_id in pages.keys():\n",
    "            query_revisions = pages[page_id][\"revisions\"]\n",
    "            title = pages[page_id]['title']\n",
    "            \n",
    "            for rev in query_revisions:\n",
    "                if \"userhidden\" in rev:\n",
    "                    continue\n",
    "                    \n",
    "                rev[\"title\"] = title # add a key called title \n",
    "                \n",
    "                if \"anon\" in rev:\n",
    "                    rev[\"anon\"] = True\n",
    "                else:\n",
    "                    rev[\"anon\"] = False\n",
    "                    \n",
    "                if \"minor\" in rev:\n",
    "                    rev[\"minor\"] = True\n",
    "                else:\n",
    "                    rev[\"minor\"] = False\n",
    "                    \n",
    "                rev[\"timestamp\"] = rev[\"timestamp\"].replace(\"T\", \"\")\n",
    "                rev[\"timestamp\"] = rev[\"timestamp\"].replace(\"Z\", \"\")\n",
    "                \n",
    "                revisions.append(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41f9c4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33854"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_edits = len(revisions)\n",
    "num_edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c21e857b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35682637206829326% of the edits were made by users without accounts\n"
     ]
    }
   ],
   "source": [
    "# anonymous edits \n",
    "\n",
    "num_anon = 0\n",
    "\n",
    "for rev in revisions:\n",
    "    if rev[\"anon\"]:\n",
    "        num_anon = num_anon + 1\n",
    "        \n",
    "prop_anon = num_anon / num_edits\n",
    "\n",
    "print(f\"{prop_anon}% of the edits were made by users without accounts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c33e11e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19170555916582974% of the edits were minor\n"
     ]
    }
   ],
   "source": [
    "# minor edits \n",
    "\n",
    "num_minor = 0\n",
    "\n",
    "for rev in revisions:\n",
    "    if rev[\"minor\"]:\n",
    "        num_minor = num_minor + 1\n",
    "        \n",
    "prop_minor = num_minor / num_edits\n",
    "\n",
    "print(f\"{prop_minor}% of the edits were minor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b815579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization \n",
    "\n",
    "edits_by_day = {}\n",
    "for rev in revisions:\n",
    "    day_string = rev['timestamp'][0:10]\n",
    "\n",
    "    if day_string in edits_by_day.keys():\n",
    "        edits_by_day[day_string] = edits_by_day[day_string] + 1\n",
    "    else:\n",
    "        edits_by_day[day_string] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a7f9aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"EditOverTime.tsv\", \"w\", encoding='utf-8') as output_file:\n",
    "    print(\"date\\tedits\", file=output_file)\n",
    "\n",
    "    for day_string in edits_by_day.keys():\n",
    "        print(\"\\t\".join([day_string, str(edits_by_day[day_string])]), file=output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f724b",
   "metadata": {},
   "source": [
    "https://docs.google.com/spreadsheets/d/1YJYz4E7tth8NZoIApH6S3sfR7CGt1OZyU2NWgmHWes4/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c557b6",
   "metadata": {},
   "source": [
    "FANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "65db5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting data on Alien Franchise from Fandom API\n",
    "\n",
    "def get_article_revision_json(title):\n",
    "    api2_answers = []\n",
    "    \n",
    "    fandom_api_url = \"https://alienanthology.fandom.com/api.php\" \n",
    "    \n",
    "    parameters = {'action' : 'query',\n",
    "                  'titles' : title,\n",
    "                  'prop' : 'revisions',\n",
    "                  'rvprop' : 'flags|timestamp|user|size|ids',\n",
    "                  'rvlimit' : 500,\n",
    "                  'format' : 'json'\n",
    "                   }\n",
    "    \n",
    "    while True: \n",
    "        \n",
    "        call = requests.get(fandom_api_url, params=parameters)\n",
    "        api2_answer = call.json()\n",
    "        \n",
    "        api2_answers.append(api2_answer)\n",
    "        \n",
    "        if 'continue' in api2_answer.keys():\n",
    "            parameters.update(api2_answer['continue'])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return(api2_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22f6c513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now working on: Alien (franchise)\n",
      "now working on: Untitled Alien film\n",
      "now working on: List of Alien (franchise) novels\n",
      "now working on: Alien vs. Predator\n",
      "now working on: Predator (franchise)\n",
      "now working on: Prometheus (2012 film)\n",
      "now working on: Alien: Covenant\n",
      "now working on: Alien vs. Predator (film)\n",
      "now working on: Aliens (film)\n",
      "now working on: Alien (film)\n"
     ]
    }
   ],
   "source": [
    "# reading the file and adding the new data to it\n",
    "with open('alien_pages.jsonl', 'r') as input_file,\\\n",
    "    open(\"alienPages_revisions.jsonl\", 'w') as output_file:\n",
    "    \n",
    "    for line in input_file.readlines():\n",
    "        line_dict = json.loads(line)\n",
    "        page_title = line_dict[\"title\"]\n",
    "        \n",
    "        print(f\"now working on: {page_title}\")\n",
    "        api2_answers = get_article_revision_json(page_title)\n",
    "        for api2_answer in api2_answers:\n",
    "            print(json.dumps(api2_answer), file=output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4a7d284",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'revisionsFantom'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# for every page, (there should always be only one) get its revisions:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_id \u001b[38;5;129;01min\u001b[39;00m pages\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 12\u001b[0m     query_revisions \u001b[38;5;241m=\u001b[39m \u001b[43mpages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpage_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevisionsFantom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m     title \u001b[38;5;241m=\u001b[39m pages[page_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# for every revision, first we do some cleaning up\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'revisionsFantom'"
     ]
    }
   ],
   "source": [
    "# revisions that failed this time (idk why)\n",
    "revisionsFantom = []\n",
    "\n",
    "with open(\"alienPages_revisions.jsonl\", 'r') as input_file:\n",
    "    for line in input_file.readlines():\n",
    "        api_answer = json.loads(line)\n",
    "        \n",
    "        # get the list of pages from the json object\n",
    "        pages = api_answer[\"query\"][\"pages\"]\n",
    "\n",
    "        # for every page, (there should always be only one) get its revisions:\n",
    "        for page_id in pages.keys():\n",
    "            query_revisions = pages[page_id][\"revisionsFantom\"]\n",
    "            title = pages[page_id]['title']\n",
    "\n",
    "            # for every revision, first we do some cleaning up\n",
    "            for rev in query_revisions:\n",
    "                #print(rev)\n",
    "                # let's continue/skip this revision if the user is hidden\n",
    "                if \"userhidden\" in rev.keys():\n",
    "                    continue\n",
    "                \n",
    "                # 1: add a title field for the article because we're going to mix them together\n",
    "                rev[\"title\"] = title\n",
    "\n",
    "                # 2: let's \"recode\" anon so it's true or false instead of present/missing\n",
    "                if \"anon\" in rev.keys():\n",
    "                    rev[\"anon\"] = True\n",
    "                else:\n",
    "                    rev[\"anon\"] = False\n",
    "\n",
    "                # 3: let's recode \"minor\" in the same way\n",
    "                if \"minor\" in rev.keys():\n",
    "                    rev[\"minor\"] = True\n",
    "                else:\n",
    "                    rev[\"minor\"] = False\n",
    "\n",
    "                # we're going to change the timestamp to make it work a little better in excel/spreadsheets\n",
    "                rev[\"timestamp\"] = rev[\"timestamp\"].replace(\"T\", \" \")\n",
    "                rev[\"timestamp\"] = rev[\"timestamp\"].replace(\"Z\", \"\")\n",
    "\n",
    "                # finally, save the revisions we've seen to a varaible\n",
    "                revisions.append(rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aef139",
   "metadata": {},
   "source": [
    "Hmmm isn't this the same code that mako used and I used for the other one before? What's wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f5ee19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7d038ed",
   "metadata": {},
   "source": [
    "3_ Progress on Final Project\n",
    "\n",
    "1- my project's end result will not be a data visulization table, I will use the data I collect to generate speculative data.\n",
    "\n",
    "2- I will collect written data on Islamic Artifacts and Visual data on them.\n",
    "\n",
    "3_ a) I think only one call from my API's will provide me with more that enough data to start with\n",
    "   b) I'm not using this but I think something similar to what Mako showed with the continue in the code will be used for this (probably depending on the API documentation)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8fc02e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as islamic_artifacts.jsonl successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_islamic_artifacts():\n",
    "    search_term = \"Islamic artifacts\"\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?action=query&format=json&list=search&srsearch={search_term}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    artifacts = []\n",
    "    \n",
    "    if \"query\" in data and \"search\" in data[\"query\"]:\n",
    "        search_results = data[\"query\"][\"search\"]\n",
    "        \n",
    "        for result in search_results:\n",
    "            title = result[\"title\"]\n",
    "            snippet = result[\"snippet\"]\n",
    "            page_id = result[\"pageid\"]\n",
    "            \n",
    "            # Create a dictionary for each artifact and add it to the list\n",
    "            artifact_info = {\n",
    "                \"title\": title,\n",
    "                \"snippet\": snippet,\n",
    "                \"page_id\": page_id\n",
    "            }\n",
    "            artifacts.append(artifact_info)\n",
    "    \n",
    "    return artifacts\n",
    "\n",
    "# Call the function to get the list of Islamic artifacts\n",
    "islamic_artifacts = get_islamic_artifacts()\n",
    "\n",
    "# Save the data to a JSONL file\n",
    "with open(\"islamic_artifacts.jsonl\", \"w\") as jsonl_file:\n",
    "    for artifact in islamic_artifacts:\n",
    "        jsonl_file.write(json.dumps(artifact) + \"\\n\")\n",
    "\n",
    "print(\"Data saved as islamic_artifacts.jsonl successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc56ad8",
   "metadata": {},
   "source": [
    "5_ 3.1 kB\n",
    "This is only for a couple of pages that this call collected for me, the file I'll be using will probably be around 4mB (based on the data texts that i have collected in my previous projects?) or more. I don't think this is going to be a problem since this is not really a very large file for python. (I assume)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
